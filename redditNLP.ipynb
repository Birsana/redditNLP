{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "redditNLP.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyPS/CMIWSvEcAWDwrNrCzTq"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "JBZLqi2OEpFK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "import time\n",
        "import requests\n",
        "import json\n",
        "import datetime\n",
        "import tensorflow as tf\n",
        "\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers"
      ],
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lqsbup0_Ffnh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dates_list = []\n",
        "\n",
        "# Making list of date ranges to use (from 2018 to 2020)\n",
        "for i in range(18,21):\n",
        "    dates_list.append('01/01/20'+str(i))\n",
        "    dates_list.append('01/6/20'+str(i))\n",
        "\n",
        "#convert them to a timestamp for pushshift API\n",
        "def convertToTimestamp(date):\n",
        "    return time.mktime(datetime.datetime.strptime(date, \"%d/%m/%Y\").timetuple())\n",
        "\n",
        "dates = [int(convertToTimestamp(date)) for date in dates_list]"
      ],
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MKWcpjadGvXU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def getRedditData(after, before, subreddit):\n",
        "  #use pushshift api to get subreddit data\n",
        "    url = ('https://api.pushshift.io/reddit/search/submission/?size=1500&after='+\n",
        "           str(after)+'&before='+str(before)+'&subreddit='+str(subreddit)+'&sort_type=score'+'&sort=desc')\n",
        "    r = requests.get(url)\n",
        "    data = json.loads(r.text)\n",
        "    return data['data']"
      ],
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gF7YFRp1H412",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "debd8daf-dbd3-4c8e-d816-bf0b12215871"
      },
      "source": [
        "def getTitles(subreddit):\n",
        "  #get just the post titles\n",
        "    titles_new = []\n",
        "    titles = []\n",
        "\n",
        "    for i in range(len(dates)-1):\n",
        "        #the date range we are fetching data for\n",
        "        after  = dates[i]\n",
        "        before = dates[i+1]\n",
        "\n",
        "        # Getting subreddit data between the dates after and before\n",
        "        raw_json = getRedditData(after,before,subreddit)\n",
        "\n",
        "        # Extracting just the title\n",
        "        titles_new = [post['title'] for post in raw_json]\n",
        "\n",
        "        # Appending new data on\n",
        "        titles = titles + titles_new\n",
        "\n",
        "    # Use set to get rid of duplicates\n",
        "    titles = list(set(titles))\n",
        "    return titles\n",
        "\n",
        "conservative = getTitles('conservative')\n",
        "liberal = getTitles('liberal')\n",
        "\n",
        "print(len(conservative))\n",
        "print(len(liberal))"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "494\n",
            "497\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZLvN2IyCk3KG",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "1972c801-05bc-4443-d76c-9c98c1d3d2ef"
      },
      "source": [
        "df1= pd.DataFrame({'text':conservative})\n",
        "df1['label'] = 1 #set 1 to conservative posts\n",
        "\n",
        "df2 = pd.DataFrame({'text':liberal})\n",
        "df2['label'] = 0 #set 0 to liberal posts\n",
        "\n",
        "# Combining both datasets\n",
        "df = pd.concat([df1,df2])\n",
        "\n",
        "# Shuffling the dataset\n",
        "df = df.sample(frac=1).reset_index(drop=True)\n",
        "\n",
        "# Converting all text to lowercase, fixing ampersands and getting rid\n",
        "# of dashes and apostrophes as they can mess up the dictionary\n",
        "df['text'] = df['text'].str.lower()\n",
        "df['text'] = df['text'].str.replace(r'&amp;', 'and')\n",
        "df['text'] = df['text'].str.replace(r'-', ' ')\n",
        "df['text'] = df['text'].str.replace(r'[^\\s\\w]','')\n",
        "\n",
        "df.head()"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>navy seal make a wish</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>vindman says white house deleted trumps refere...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>republicans defend trump argument that accepti...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>democrats alarmed by trumps promise of pardons...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>mike rowe americas suffering from an epidemic ...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                text  label\n",
              "0                              navy seal make a wish      1\n",
              "1  vindman says white house deleted trumps refere...      0\n",
              "2  republicans defend trump argument that accepti...      0\n",
              "3  democrats alarmed by trumps promise of pardons...      0\n",
              "4  mike rowe americas suffering from an epidemic ...      1"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sr6Njj7Gs4SR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "vocab_set = set()\n",
        "sentence_lengths = []\n",
        "\n",
        "for i in range(len(df)):\n",
        "    # Get the words from the text, and the lengths of the sentences\n",
        "    sentence_words = re.split(r'\\s',df.iloc[i]['text'])\n",
        "    vocab_set.update(sentence_words)\n",
        "    sentence_lengths.append(len(sentence_words))"
      ],
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zvNYR66Ts6Ps",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#get the unique words\n",
        "vocab_list = list(vocab_set)\n",
        "vocab_dict = {vocab_list[i-1]: i for i in range(1, len(vocab_list)+1)}"
      ],
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fkJxSLbGs-jF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "max_length = max(sentence_lengths)\n",
        "#map words ot numbers\n",
        "def toNumbers(row):\n",
        "    words = re.findall(r'([\\w]+)', row['text'])\n",
        "    nums =  np.array([vocab_dict[words[j]] for j in range(len(words))])\n",
        "    #pad to max length\n",
        "    return np.pad(nums, (0, max_length - len(nums)), mode='constant')"
      ],
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kvrWStlFtAsq",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        },
        "outputId": "b596f57d-c13c-45a5-8661-acf5468c53d7"
      },
      "source": [
        "nums = df.apply(lambda row: toNumbers(row), axis=1) \n",
        "df['nums'] = nums\n",
        "\n",
        "df['nums'].head()"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0    [808, 2073, 3236, 276, 1867, 0, 0, 0, 0, 0, 0,...\n",
              "1    [3225, 3449, 3678, 932, 759, 3669, 265, 1010, ...\n",
              "2    [2703, 110, 1202, 2643, 3670, 134, 3344, 2485,...\n",
              "3    [200, 901, 2873, 3669, 11, 1831, 667, 1010, 31...\n",
              "4    [2701, 54, 2128, 685, 2385, 163, 256, 1831, 28...\n",
              "Name: nums, dtype: object"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fgO08vFvtE0A",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "805f1ce3-62cd-4754-8fe7-49f6a8c6d058"
      },
      "source": [
        "labels = np.asarray(df['label'].values)\n",
        "features = np.stack(df['nums'].values)\n",
        "\n",
        "features.shape, labels.shape"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((991, 51), (991,))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xkPEfpDHtwQ8",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        },
        "outputId": "2c268c5a-01bf-48fc-9d87-f5b1dda4c3a3"
      },
      "source": [
        "def get_compiled_model():\n",
        "  #fiddling around with the model more should allow for higher accuracy, right\n",
        "  #now it is around 83%\n",
        "    embedding_dim=16\n",
        "\n",
        "    model = tf.keras.Sequential([\n",
        "        tf.keras.layers.Embedding(len(vocab_set)+1, 64), #embedding layer important for NLP\n",
        "        tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64,  return_sequences=True)),\n",
        "        tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(32)),\n",
        "        tf.keras.layers.Dense(64, activation='relu'),\n",
        "        tf.keras.layers.Dropout(0.4),\n",
        "        tf.keras.layers.Dense(1, activation='sigmoid')\n",
        "    ])\n",
        "\n",
        "\n",
        "    model.compile(optimizer='adam',\n",
        "                loss='binary_crossentropy',\n",
        "                metrics=['accuracy'])\n",
        "    \n",
        "    return model\n",
        "\n",
        "model = get_compiled_model()\n",
        "model.fit(features, labels, batch_size=32, epochs=4, verbose=2, validation_split=0.2);\n"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/4\n",
            "25/25 - 4s - loss: 0.6588 - accuracy: 0.6301 - val_loss: 0.5727 - val_accuracy: 0.6734\n",
            "Epoch 2/4\n",
            "25/25 - 2s - loss: 0.4710 - accuracy: 0.7967 - val_loss: 0.4362 - val_accuracy: 0.8040\n",
            "Epoch 3/4\n",
            "25/25 - 2s - loss: 0.2254 - accuracy: 0.9356 - val_loss: 0.5248 - val_accuracy: 0.8141\n",
            "Epoch 4/4\n",
            "25/25 - 2s - loss: 0.0772 - accuracy: 0.9798 - val_loss: 0.6356 - val_accuracy: 0.8141\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}